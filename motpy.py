# -*- coding: utf-8 -*-
"""motpy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NsgOseS9gB1x4BnJ0TrzIw8ebJISGOS0
"""

!pip install motpy

#SSDとmotpy
import copy

import cv2
import numpy as np
import torch
import torchvision
from torchvision.models.detection import (
    SSDLite320_MobileNet_V3_Large_Weights,
    ssdlite320_mobilenet_v3_large,
)

from motpy import Detection, MultiObjectTracker
from google.colab.patches import cv2_imshow

def get_id_color(index):
    temp_index = (index + 1) * 5
    color = (
        (37 * temp_index) % 255,
        (17 * temp_index) % 255,
        (29 * temp_index) % 255,
    )
    return color


def draw_debug(
    image,
    track_results,
    track_id_dict,
    coco_classes,
):
    debug_image = copy.deepcopy(image)

    for track_result in track_results:
        tracker_id = track_id_dict[track_result.id]
        bbox = track_result.box
        class_id = int(track_result.class_id)
        score = track_result.score

        x1, y1, x2, y2 = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])

        # トラッキングIDに応じた色を取得
        color = get_id_color(tracker_id)

        # バウンディングボックス描画
        debug_image = cv2.rectangle(
            debug_image,
            (x1, y1),
            (x2, y2),
            color,
            thickness=2,
        )

        # スコア、ラベル名描画
        score = '%.2f' % score
        text = '%s:%s' % (str(coco_classes[int(class_id - 1)]), score)
        debug_image = cv2.putText(
            debug_image,
            text,
            (x1, y1 - 10),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.7,
            color,
            thickness=2,
        )

    return debug_image


# OpenCV動画読み込み準備
#cap = cv2.VideoCapture('demo.mp4')
cap = cv2.VideoCapture('/content/demo.mp4')
# 物体検出(MobileNetV3 SSDLite)準備
device = 'cpu'
weights = SSDLite320_MobileNet_V3_Large_Weights.DEFAULT
detector = ssdlite320_mobilenet_v3_large(weights=weights)
detector = detector.eval().to(device)

# 物体検出前処理準備
input_transform = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
])

# クラス名読み込み
with open('/content/coco_classes_for_torchvision.txt', 'rt') as f:
    coco_classes = f.read().rstrip('\n').split('\n')

# motpy準備
fps = 30
tracker = MultiObjectTracker(
    dt=(1 / fps),
    tracker_kwargs={'max_staleness': 5},
    model_spec={
        'order_pos': 1,
        'dim_pos': 2,
        'order_size': 0,
        'dim_size': 2,
        'q_var_pos': 5000.0,
        'r_var_pos': 0.1
    },
    matching_fn_kwargs={
        'min_iou': 0.25,
        'multi_match_min_iou': 0.93
    },
)

# トラッキングID保持用変数
track_id_dict = {}


while True:
    # 動画からフレームを読み込む
    ret, frame = cap.read()
    if not ret:
        break
    debug_image = copy.deepcopy(frame)

    # 物体検出実行
    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    frame = input_transform(frame).to(device).unsqueeze(0)
    with torch.no_grad():
        result = detector(frame)

    # 物体検出実行結果からバウンディングボックス、スコア、ラベルIDを取得
    boxes, scores, labels = [
        result[0][attr].detach().cpu().numpy()
        for attr in ['boxes', 'scores', 'labels']
    ]

    # スコア閾値以上の結果のみを取得する
    score_th = 0.7
    score_indexes = np.where(scores > score_th)

    boxes = boxes[score_indexes]
    scores = scores[score_indexes]
    labels = labels[score_indexes]

    # motpy入力用のDetectionクラスにデータを設定する
    detections = [
        Detection(box=b, score=s, class_id=l)
        for b, s, l in zip(boxes, scores, labels)
    ]

    # motpyを用いてトラッキングを実行する
    _ = tracker.step(detections=detections)
    track_results = tracker.active_tracks(min_steps_alive=3)

    for track_result in track_results:
        if track_result.id not in track_id_dict:
            new_id = len(track_id_dict)
            track_id_dict[track_result.id] = new_id

    # 結果を描画する
    debug_image = draw_debug(
        debug_image,
        track_results,
        track_id_dict,
        coco_classes,
    )


    # キー処理(ESC：終了)
    key = cv2.waitKey(1)
    if key == 27:  # ESC
        break

    # 画面反映
    cv2_imshow(debug_image)

cap.release()
cv2.destroyAllWindows()

!pip install onnxruntime

#YOLOXとmotpy
import sys
ROOT_PATH = '/content/yolox'
sys.path.append(ROOT_PATH)

import copy

import cv2
from motpy import Detection, MultiObjectTracker

from yolox.yolox_onnx import YoloxONNX


def get_id_color(index):
    temp_index = (index + 1) * 5
    color = (
        (37 * temp_index) % 255,
        (17 * temp_index) % 255,
        (29 * temp_index) % 255,
    )
    return color


def draw_debug(
    image,
    track_results,
    track_id_dict,
):
    debug_image = copy.deepcopy(image)

    for track_result in track_results:
        tracker_id = track_id_dict[track_result.id]
        bbox = track_result.box
        class_id = int(track_result.class_id)
        score = track_result.score

        x1, y1, x2, y2 = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])

        # トラッキングIDに応じた色を取得
        color = get_id_color(tracker_id)

        # バウンディングボックス描画
        debug_image = cv2.rectangle(
            debug_image,
            (x1, y1),
            (x2, y2),
            color,
            thickness=2,
        )

        # スコア、ラベル名描画
        score = '%.2f' % score
        text = '%s' % (score)
        debug_image = cv2.putText(
            debug_image,
            text,
            (x1, y1 - 10),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.7,
            color,
            thickness=2,
        )

    return debug_image


# OpenCV動画読み込み準備
cap = cv2.VideoCapture('/content/fish.mp4')

# 物体検出(YOLOX-Nano)準備
yolox = YoloxONNX(
    model_path='/content/yolox_nano.onnx',
    input_shape=(416, 416),
    class_score_th=0.3,
    nms_th=0.45,
    nms_score_th=0.1,
    with_p6=False,
)

# motpy準備
fps = 30
tracker = MultiObjectTracker(
    dt=(1 / fps),
    tracker_kwargs={'max_staleness': 5},
    model_spec={
        'order_pos': 1,
        'dim_pos': 2,
        'order_size': 0,
        'dim_size': 2,
        'q_var_pos': 5000.0,
        'r_var_pos': 0.1
    },
    matching_fn_kwargs={
        'min_iou': 0.25,
        'multi_match_min_iou': 0.93
    },
)

# トラッキングID保持用変数
track_id_dict = {}

while True:
    # 動画からフレームを読み込む
    ret, frame = cap.read()
    if not ret:
        break
    debug_image = copy.deepcopy(frame)

    # 物体検出実行
    boxes, scores, labels = yolox.inference(frame)

    # motpy入力用のDetectionクラスにデータを設定する
    detections = [
        Detection(box=b, score=s, class_id=l)
        for b, s, l in zip(boxes, scores, labels)
    ]

    # motpyを用いてトラッキングを実行する
    _ = tracker.step(detections=detections)
    track_results = tracker.active_tracks(min_steps_alive=3)

    # トラッキングIDと連番の紐付け
    for track_result in track_results:
        if track_result.id not in track_id_dict:
            new_id = len(track_id_dict)
            track_id_dict[track_result.id] = new_id

    # 結果を描画する
    debug_image = draw_debug(
        debug_image,
        track_results,
        track_id_dict,
    )

    # キー処理(ESC：終了)
    key = cv2.waitKey(1)
    if key == 27:  # ESC
        break

    # 画面反映
    cv2_imshow(debug_image)

cap.release()
cv2.destroyAllWindows()

